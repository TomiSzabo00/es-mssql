{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":true,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Elasticsearch & MSSQL \u00b6 This repository contains laboratory exercises for the following course topics: Elasticsearch laboratory Microsoft SQL Server Business Intelligence laboratory For the other laboratory topics, please refer to the course website. Submitting your work \u00b6 You need to submit your solutions in git repositories. Please refer to the detailed guideline . IMPORTANT You are required to follow these guidelines. Exercises not submitted in the expected format are not graded. Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized. Screenshots \u00b6 The exercises will ask for a few screenshots. The exercises describe the expected content of these screenshots. You can create a screenshot of the entire desktop or the specified application. You should upload the screenshots to the git repository as part of your solution. The repository is private, hence only you and the instructors have access to it. Automatic evaluation \u00b6 You will receive automatic feedback in GitHub when submitting your solution. This feedback includes a \"total,\" which is the number of successfully completed exercises; this is a preliminary grade, and it is subject to change by the instructor. Your solution is expected to work in this automated evaluation environment . Please take extra care following the exercise guidelines. The automated feedback will let you know if there is some error in your submission; you must fix these. The evaluation uses GitHub Actions . You have access to all the logs and the detailed result of every evaluation on the GitHub web interface. This guide helps you understand where to look for them. Pull requests welcome As a student of this course, you can earn extra credit by contributing to the materials! Open a pull request to fix an error or contribute to the contents in any way! Check the link to the repository in the upper right corner. License The materials found in this repository are created for the students of course BMEVIAUMB00. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed. The materials are to be used within the course context. For any other usage scenarios, the material is provided as-is.","title":"Elasticsearch & MSSQL"},{"location":"#elasticsearch-mssql","text":"This repository contains laboratory exercises for the following course topics: Elasticsearch laboratory Microsoft SQL Server Business Intelligence laboratory For the other laboratory topics, please refer to the course website.","title":"Elasticsearch &amp; MSSQL"},{"location":"#submitting-your-work","text":"You need to submit your solutions in git repositories. Please refer to the detailed guideline . IMPORTANT You are required to follow these guidelines. Exercises not submitted in the expected format are not graded. Workflow errors, i.e., not following the guidelines (e.g., not assigning the right person, or not assigning at all) are penalized.","title":"Submitting your work"},{"location":"#screenshots","text":"The exercises will ask for a few screenshots. The exercises describe the expected content of these screenshots. You can create a screenshot of the entire desktop or the specified application. You should upload the screenshots to the git repository as part of your solution. The repository is private, hence only you and the instructors have access to it.","title":"Screenshots"},{"location":"#automatic-evaluation","text":"You will receive automatic feedback in GitHub when submitting your solution. This feedback includes a \"total,\" which is the number of successfully completed exercises; this is a preliminary grade, and it is subject to change by the instructor. Your solution is expected to work in this automated evaluation environment . Please take extra care following the exercise guidelines. The automated feedback will let you know if there is some error in your submission; you must fix these. The evaluation uses GitHub Actions . You have access to all the logs and the detailed result of every evaluation on the GitHub web interface. This guide helps you understand where to look for them. Pull requests welcome As a student of this course, you can earn extra credit by contributing to the materials! Open a pull request to fix an error or contribute to the contents in any way! Check the link to the repository in the upper right corner. License The materials found in this repository are created for the students of course BMEVIAUMB00. The usage of these materials outside the scope of teaching or learning this particular course is only granted if the source and authors are contributed. The materials are to be used within the course context. For any other usage scenarios, the material is provided as-is.","title":"Automatic evaluation"},{"location":"FAQ/","text":"FAQ \u00b6 If you face a problem, you may find a solution below. You cannot find your repository \u00b6 Go to www.github.com , and after login, you will see the list of your repositories on the left. \"Run failed\" / \"Checks have failed\" \u00b6 After each commit you push, a minimal set of tests are executed for verification. This email tells you that these checks have failed. You must fix these. In the email, you will find a link \"View results.\" In the pull request, there is a \"Details\" link. You can check the logs of the execution here and look for red X-es marking the errors. You do not know that is the problem in your solution \u00b6 It is not our goal with the evaluation to explain the problems precisely. It is part of your work to evaluate and test your own solution. The error message can also come from the actual execution of your work and comes from the tools used. You finished your work but got no evaluation/grade \u00b6 Did you: commit? Push? Open the pull request? Assign it?","title":"FAQ"},{"location":"FAQ/#faq","text":"If you face a problem, you may find a solution below.","title":"FAQ"},{"location":"FAQ/#you-cannot-find-your-repository","text":"Go to www.github.com , and after login, you will see the list of your repositories on the left.","title":"You cannot find your repository"},{"location":"FAQ/#run-failed-checks-have-failed","text":"After each commit you push, a minimal set of tests are executed for verification. This email tells you that these checks have failed. You must fix these. In the email, you will find a link \"View results.\" In the pull request, there is a \"Details\" link. You can check the logs of the execution here and look for red X-es marking the errors.","title":"\"Run failed\" / \"Checks have failed\""},{"location":"FAQ/#you-do-not-know-that-is-the-problem-in-your-solution","text":"It is not our goal with the evaluation to explain the problems precisely. It is part of your work to evaluate and test your own solution. The error message can also come from the actual execution of your work and comes from the tools used.","title":"You do not know that is the problem in your solution"},{"location":"FAQ/#you-finished-your-work-but-got-no-evaluationgrade","text":"Did you: commit? Push? Open the pull request? Assign it?","title":"You finished your work but got no evaluation/grade"},{"location":"GitHub-Actions/","text":"GitHub Actions \u00b6 The automatic evaluation of your submissions uses GitHub Actions . It is a CI/CD and workflow automation platform that enables us to execute and test your code in a virtual environment. The evaluation will give you feedback in your pull request. In case you need more information or want to see the application logs, navigate to your GitHub repository in a web browser and click Actions . The interface will list so-called Workflows . Every execution will be a new item in this list (thus, it also gives you access to past executions and their logs). Selecting one of these (e.g., the topmost is always the last one) gives you the details. To get the logs, you need to click one more on the left, and then the logs will be on the right. Every green checkmark is a successful step in the process. These steps do not correspond to your exercises; these are the steps of the evaluation process, such as the initialization of the environment. Generally, each task shall succeed, even if there are errors in your submission. These errors should be expected and handled by the process. There is only one exception to this rule: if neptun.txt is missing, the evaluation will not proceed if this file is not correct. There can be transient errors though, such as a temporary network or git error. In such case, you can re-run the evaluation. Naturally, re-running will only resolve transient errors. You can deduce the nature of the problem from the step name and the log itself. You can also look at the execution log of the evaluation. In the case of Microsoft SQL Server Integration Services, this will provide you with the ETL execution log. (In other cases, the logs can be different.)","title":"GitHub Actions"},{"location":"GitHub-Actions/#github-actions","text":"The automatic evaluation of your submissions uses GitHub Actions . It is a CI/CD and workflow automation platform that enables us to execute and test your code in a virtual environment. The evaluation will give you feedback in your pull request. In case you need more information or want to see the application logs, navigate to your GitHub repository in a web browser and click Actions . The interface will list so-called Workflows . Every execution will be a new item in this list (thus, it also gives you access to past executions and their logs). Selecting one of these (e.g., the topmost is always the last one) gives you the details. To get the logs, you need to click one more on the left, and then the logs will be on the right. Every green checkmark is a successful step in the process. These steps do not correspond to your exercises; these are the steps of the evaluation process, such as the initialization of the environment. Generally, each task shall succeed, even if there are errors in your submission. These errors should be expected and handled by the process. There is only one exception to this rule: if neptun.txt is missing, the evaluation will not proceed if this file is not correct. There can be transient errors though, such as a temporary network or git error. In such case, you can re-run the evaluation. Naturally, re-running will only resolve transient errors. You can deduce the nature of the problem from the step name and the log itself. You can also look at the execution log of the evaluation. In the case of Microsoft SQL Server Integration Services, this will provide you with the ETL execution log. (In other cases, the logs can be different.)","title":"GitHub Actions"},{"location":"GitHub-credentials/","text":"GitHub credentials in university labs \u00b6 In the university laboratories, the computer may remember your login information. To remove your credentials, follow these steps. Open Credential Manager from the Start menu. Look for GitHub tokens in the Windows Credentials page, and remove all of them.","title":"GitHub credentials in university labs"},{"location":"GitHub-credentials/#github-credentials-in-university-labs","text":"In the university laboratories, the computer may remember your login information. To remove your credentials, follow these steps. Open Credential Manager from the Start menu. Look for GitHub tokens in the Windows Credentials page, and remove all of them.","title":"GitHub credentials in university labs"},{"location":"GitHub/","text":"Submitting your work (GitHub) \u00b6 We are using GitHub to submit the solutions. You submit each laboratory in a GitHub repository that you will create through a provided link. The solution of the laboratory exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the laboratory instructor with GitHub name akosdudas . IMPORTANT The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded. Short version, aka. TL;DR \u00b6 The detailed description below shows the entire procedure. This summary is an overview of the whole process. The lab exercises are solved in a dedicated GitHub repository created through a link in the exercise description. Your solution is submitted on a new branch, not on master. You can create any number of commits on this branch. You need to push these commits to GitHub. You submit your final solution through a pull request assigned to the laboratory instructor. You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your lab instructor use the @name annotation in the comment text. Starting your work: git checkout \u00b6 Register a GitHub account if you don't have one yet. Open the link in the exercise description to create your repository. If needed, authorize the GitHub Classroom application to use your account data. You will see a page where you can \"Accept the ... assignment\". Click the button. Wait for the repository creation to finish. You will get the repository URL here. Note The repository will be private. No one but you and the instructor will see it. Open the repository webpage by following the link. You will need this URL, so remember it. Clone the repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button. You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub. If you are using the console or the shell, the following command performs the clone (if the git command is available): git clone <repository link> If you are using Sourcetree If you are using Sourcetree, you might need to use a Personal Access Token (PAT) for access as follows: Head to your GitHub account settings and choose Developer settings from the left menu. Choose the Personal access token option and click Generate new token . The note field is to remember what this token was created for. Let's give a name, e.g. sourcetree . At the Select scope form tick every box available and click Generate token . After the generation process copy the created token to the clipboard and head over to the Sourcetree app. Go to Tools \u2192 Options \u2192 Authentication. If you already have a GitHub account set up with some other access token or authentication info, click on it and choose Edit otherwise choose Add . Choose the hosting service as GitHub, preferred protocol as HTTPS, and at the Credentials part choose Authentication Basic . Type in your GitHub username then click on the Refresh Password option. Copy the created token as the password and click OK. After this, cloning and pushing to the repository should work. If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository master branch. Instead, create a new branch with the name solution . In GitHub Desktop, use the Branch menu for creating a new one. If using the console, use the following command: git checkout -b solution Complete the exercises on this branch. You may have any number of commits and pushes. In university labs Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands. git config user.name git config user.email If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.) git config user.name \"John Doe\" git config user.email \"john@doe.org\" At home When working from home, you may want to set the name and email address globally using the --global switch in the commands above. To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published. When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing. If you are using the console, use the following commands: # Check the current branch and the files modified git status # Prepares all changes for commit git add . # Commit git commit -m \"f1\" # Push the new branch (first time) git push --set-upstream origin solution # Push further commits git push Submitting the solution \u00b6 When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches. GitHub web file upload We recommend that you do not use the GitHub web file upload. If something is missing, add it to your local repository and commit and push again. When you are truly ready, open a pull request . Why the pull request? This pull request combines all changes you made and shows us the final result. This helps the laboratory instructor to evaluate your submission more easily. This pull request means you submit your solution; hence this step cannot be omitted . To open the pull request , you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request. You may also open the pull request from the menu at the top. It is important to specify the correct branches: master is the target into which solution is merged. When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET! The pull request will trigger a preliminary evaluation. You will see the result in a comment in the pull request thread. This evaluation may differ from the image. And it may take some time. If you need more information about the evaluation and the results, GitHub Actions can provide you more. See the short guide here . If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request . We ask that you trigger NO MORE THAN 5 evaluations ! Making further changes without running the evaluation If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft . This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation. Maximum 5 Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it. FINALLY , when you are ready, assign the pull request to your laboratory instructor. This step is considered as the submission of your work. Without pull request If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted. Done Now you are ready. After assigning the pull request, make no further changes . The instructor will evaluate the submission and close the pull request. Questions and complaints regarding the final result \u00b6 If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use @akosdudas mention in the PR comment. This will automatically send an email notification. Please provide proof Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).","title":"Submitting your work (GitHub)"},{"location":"GitHub/#submitting-your-work-github","text":"We are using GitHub to submit the solutions. You submit each laboratory in a GitHub repository that you will create through a provided link. The solution of the laboratory exercises are created within these repositories, then committed and pushed to GitHub. The submission is finished with a pull request assigned to the laboratory instructor with GitHub name akosdudas . IMPORTANT The submission requirements detailed below and mandatory. Submissions not following these guidelines are not graded.","title":"Submitting your work (GitHub)"},{"location":"GitHub/#short-version-aka-tldr","text":"The detailed description below shows the entire procedure. This summary is an overview of the whole process. The lab exercises are solved in a dedicated GitHub repository created through a link in the exercise description. Your solution is submitted on a new branch, not on master. You can create any number of commits on this branch. You need to push these commits to GitHub. You submit your final solution through a pull request assigned to the laboratory instructor. You can ask questions regarding the results and evaluation in the pull request comment thread. To notify your lab instructor use the @name annotation in the comment text.","title":"Short version, aka. TL;DR"},{"location":"GitHub/#starting-your-work-git-checkout","text":"Register a GitHub account if you don't have one yet. Open the link in the exercise description to create your repository. If needed, authorize the GitHub Classroom application to use your account data. You will see a page where you can \"Accept the ... assignment\". Click the button. Wait for the repository creation to finish. You will get the repository URL here. Note The repository will be private. No one but you and the instructor will see it. Open the repository webpage by following the link. You will need this URL, so remember it. Clone the repository. You will need the repository git URL, which you can get from the repository webpage following the Clone or download button. You may use any git client. The simplest one is GitHub Desktop if you do not have a favorite yet. You can list your repositories in this application directly from GitHub. If you are using the console or the shell, the following command performs the clone (if the git command is available): git clone <repository link> If you are using Sourcetree If you are using Sourcetree, you might need to use a Personal Access Token (PAT) for access as follows: Head to your GitHub account settings and choose Developer settings from the left menu. Choose the Personal access token option and click Generate new token . The note field is to remember what this token was created for. Let's give a name, e.g. sourcetree . At the Select scope form tick every box available and click Generate token . After the generation process copy the created token to the clipboard and head over to the Sourcetree app. Go to Tools \u2192 Options \u2192 Authentication. If you already have a GitHub account set up with some other access token or authentication info, click on it and choose Edit otherwise choose Add . Choose the hosting service as GitHub, preferred protocol as HTTPS, and at the Credentials part choose Authentication Basic . Type in your GitHub username then click on the Refresh Password option. Copy the created token as the password and click OK. After this, cloning and pushing to the repository should work. If the cloning is successful, DO NOT START WORKING YET! The solution should not be committed to the repository master branch. Instead, create a new branch with the name solution . In GitHub Desktop, use the Branch menu for creating a new one. If using the console, use the following command: git checkout -b solution Complete the exercises on this branch. You may have any number of commits and pushes. In university labs Before you make your first commit, check whether your name and email address are properly configured. You can check this using the following commands. git config user.name git config user.email If the values are not correct, set your name and email address with the following commands executed in the repository directory. This will set the values for the repository. (It is recommended to set the email address to the one you use with GitHub.) git config user.name \"John Doe\" git config user.email \"john@doe.org\" At home When working from home, you may want to set the name and email address globally using the --global switch in the commands above. To commit using GitHub Desktop, first check if you are on the right branch. During the first push, the solution branch needs to be published. When adding further commits, verify the branch. You can publish the commit using the Push origin button. The little number on this button shows you how many commits need pushing. If you are using the console, use the following commands: # Check the current branch and the files modified git status # Prepares all changes for commit git add . # Commit git commit -m \"f1\" # Push the new branch (first time) git push --set-upstream origin solution # Push further commits git push","title":"Starting your work: git checkout"},{"location":"GitHub/#submitting-the-solution","text":"When you are ready with the exercises, verify on the repository web page that you uploaded everything. You may need to switch branches. GitHub web file upload We recommend that you do not use the GitHub web file upload. If something is missing, add it to your local repository and commit and push again. When you are truly ready, open a pull request . Why the pull request? This pull request combines all changes you made and shows us the final result. This helps the laboratory instructor to evaluate your submission more easily. This pull request means you submit your solution; hence this step cannot be omitted . To open the pull request , you need to go to the repository's GitHub web frontend. If you pushed recently, GitHub will offer you to create the pull request. You may also open the pull request from the menu at the top. It is important to specify the correct branches: master is the target into which solution is merged. When the pull request is created, you will see a little number \"1\" on the Pull request menu showing you that there is one open item there. YOU ARE NOT FINISHED YET! The pull request will trigger a preliminary evaluation. You will see the result in a comment in the pull request thread. This evaluation may differ from the image. And it may take some time. If you need more information about the evaluation and the results, GitHub Actions can provide you more. See the short guide here . If you are not satisfied with your work, you can make further changes. You only need to commit and push your changes. Any changes pushed will re-trigger the evaluation of the pull request . We ask that you trigger NO MORE THAN 5 evaluations ! Making further changes without running the evaluation If you want to make changes to your submission and not have the re-evaluation run, you should convert the pull request to draft . This state means work in progress. You can commit and push freely. These will not trigger any evaluation. Once ready, you must change the state back: go to the bottom of the PR and click \"Ready for review.\" This will set the PR back to its normal state and trigger an automated evaluation. Maximum 5 Evaluations that fail due to transient errors, such as network problems, are not counted into the 5 evaluations. But if you trigger more evaluation by mistake, or on purpose, it will be sanctioned. You are required to test your solution locally before submitting it. FINALLY , when you are ready, assign the pull request to your laboratory instructor. This step is considered as the submission of your work. Without pull request If you have no pull request, or it is not assigned to the instructor, we consider it work in progress and not submitted. Done Now you are ready. After assigning the pull request, make no further changes . The instructor will evaluate the submission and close the pull request.","title":"Submitting the solution"},{"location":"GitHub/#questions-and-complaints-regarding-the-final-result","text":"If you have questions on concerns regarding the automated evaluation, use the pull request for communication with the instructor by asking questions via comments. To let the instructor know you have questions, please use @akosdudas mention in the PR comment. This will automatically send an email notification. Please provide proof Please note that if you think the evaluation made a mistake, you must support your question/complaint with proof (e.g., show how you tested your solution and prove that it worked).","title":"Questions and complaints regarding the final result"},{"location":"elasticsearch/","text":"Elasticsearch laboratory \u00b6 This laboratory aims to practice working with Elasticsearch through writing queries and aggregations and try out some visualization tools provided by Kibana . Pre-requisites \u00b6 You need the following tools to complete this laboratory: Windows, Linux, or Mac: Every component you need is platform-independent. Elasticsearch and Kibana version 7.10.x Either run them using Docker Windows or Mac: install Docker Desktop Linux install Docker and install docker-compose Or you can download the binaries (see in Exercise 2) PowerShell Included in Windows Install PowerShell Core on Linux Install PowerShell Core on MacOS A GitHub account and a git tool Material to review before the laboratory \u00b6 The expected mode of submitting your work, as detailed here . The material covered in course Business intelligence related to the topic, including, but not limited to The demo material covered during the semester https://github.com/peekler/Business-Intelligence-Demos/tree/master/ELK Elasticsearch documentation for reference https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html Grading \u00b6 Exercises 1 & 2 help you prepare the environment. Starting with exercise 3, each exercise is worth one grade when completed successfully . For an exercise to count as successful , you must finish all its subtasks successfully. (E.g. if you complete exercises 3, 4 and 5 successfully, but there is an error in exercise 6, and you did not complete exercise 7, the resulting grade will be 3.)","title":"Elasticsearch laboratory"},{"location":"elasticsearch/#elasticsearch-laboratory","text":"This laboratory aims to practice working with Elasticsearch through writing queries and aggregations and try out some visualization tools provided by Kibana .","title":"Elasticsearch laboratory"},{"location":"elasticsearch/#pre-requisites","text":"You need the following tools to complete this laboratory: Windows, Linux, or Mac: Every component you need is platform-independent. Elasticsearch and Kibana version 7.10.x Either run them using Docker Windows or Mac: install Docker Desktop Linux install Docker and install docker-compose Or you can download the binaries (see in Exercise 2) PowerShell Included in Windows Install PowerShell Core on Linux Install PowerShell Core on MacOS A GitHub account and a git tool","title":"Pre-requisites"},{"location":"elasticsearch/#material-to-review-before-the-laboratory","text":"The expected mode of submitting your work, as detailed here . The material covered in course Business intelligence related to the topic, including, but not limited to The demo material covered during the semester https://github.com/peekler/Business-Intelligence-Demos/tree/master/ELK Elasticsearch documentation for reference https://www.elastic.co/guide/en/elasticsearch/reference/current/search.html https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html","title":"Material to review before the laboratory"},{"location":"elasticsearch/#grading","text":"Exercises 1 & 2 help you prepare the environment. Starting with exercise 3, each exercise is worth one grade when completed successfully . For an exercise to count as successful , you must finish all its subtasks successfully. (E.g. if you complete exercises 3, 4 and 5 successfully, but there is an error in exercise 6, and you did not complete exercise 7, the resulting grade will be 3.)","title":"Grading"},{"location":"elasticsearch/exercise1/","text":"Exercise 1: Get the starter solution \u00b6 Submission guidelines Do not forget to follow the submission guidelines . Check out the git repository \u00b6 Open the following URL and accept the assignment to get your repository: https://classroom.github.com/a/2JIfMQxz Wait for the repository creation to complete, then check out the repository. If in university computer laboratories you are not asked for credentials to log in to GitHub when checking out the repository, the operation may fail. This is likely due to the machine using someone else's GitHub credentials. Delete these credentials first (see here ), then retry the checkout. Create a new branch for the solution and work on this branch. Open the checked out folder, and type your Neptun code into the neptun.txt file. There should be a single line with the 6 characters of your Neptun code and nothing else in this file.","title":"Exercise 1: Get the starter solution"},{"location":"elasticsearch/exercise1/#exercise-1-get-the-starter-solution","text":"Submission guidelines Do not forget to follow the submission guidelines .","title":"Exercise 1: Get the starter solution"},{"location":"elasticsearch/exercise1/#check-out-the-git-repository","text":"Open the following URL and accept the assignment to get your repository: https://classroom.github.com/a/2JIfMQxz Wait for the repository creation to complete, then check out the repository. If in university computer laboratories you are not asked for credentials to log in to GitHub when checking out the repository, the operation may fail. This is likely due to the machine using someone else's GitHub credentials. Delete these credentials first (see here ), then retry the checkout. Create a new branch for the solution and work on this branch. Open the checked out folder, and type your Neptun code into the neptun.txt file. There should be a single line with the 6 characters of your Neptun code and nothing else in this file.","title":"Check out the git repository"},{"location":"elasticsearch/exercise2/","text":"Exercise 2: Initializing Elasticsearch and Kibana \u00b6 The purpose of this exercise is to start Elasticsearch and Kibana . You have two options for running them: Using Docker Or install them on your machine. In the university computer laboratories use the Docker option. Otherwise choose the option best suited for you (if you don't have Docker, you can just go with option #2). Option 1: Start Elasticsearch and Kibana using Docker \u00b6 If you have Docker, or you are working in the university computer laboratories, use this option. Start Docker Desktop and wait for it to initialize. Only in university computer laboratories : open Docker Desktop settings (right-click on the Docker icon on the taskbar), and follow the teacher's instructions to configure a proxy to be used by Docker. Open a command prompt or PowerShell console and test whether Docker is working: execute docker version , which should print version information for client and server. Locate the provided docker-compose.yml file in the checked-out starter solution (located in the root of your git repository). Open it in a text editor of your choice and check its contents. Check the following specifics: environment : This is used to set environment variables in the container. We can use these to provide some settings for Elasticsearch and Kibana as well. One example is the cluster.name variable. ulimits : In order for Elasticsearch to function properly, we have to set some ulimit values in the container. volumes : Due to how file storage works in Docker containers, every data is lost if we remove a container and start a new one. Of course, this is not exactly appropriate for a database service like Elasticsearch . Therefore we are going to use a volume which provides a more durable storage option. Navigate a command prompt / PowerShell / bash shell into the folder with the docker-compose.yml file, and issue the following commands. If you are in the university computer laboratories, please run docker-compose down -v first to remove any lingering containers and data left behind. Then to start, use docker-compose up . If you are using Linux, and this command fails, you might try to sudo to make things easier. Keep this shell open. You will see the application logs here. To shut down the running system, use CTRL + C . Your data will be kept, and restarting with the same command as above will restart the applications. To completely shut down the applications and remove all data, use docker-compose down -v . Option 2: Start Elasticsearch and Kibana by installing them \u00b6 Download the OSS version of Elasticsearch and Kibana . Please make sure to use version 7.10.x ; the starter code and the instructions might be specific to this version. Extract the archives. Extracting the zip files will take a considerable amount of time. Start both using the executables bin/elasticsearch(.bat) and bin/kibana(.bat) . There is no configuration needed; the default setup will work fine Wait for a few seconds while they start. Elasticsearch will print something like this: [INFO ][o.e.h.AbstractHttpServerTransport] [xx] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200} [INFO ][o.e.n.Node ] [xx] started Note the port where Elasticsearch is available (should be 9200). Kibana, when ready, will print: [info][server][Kibana][http] http server running at http://localhost:5601 If you need to change the ports, edit the following files, then restart the applications: config\\elasticsearch.yml , look for key http.port , uncomment it and set a port; config\\kibana.yml , look for key elasticsearch.hosts , uncomment it and set the same port. Check that the systems are running \u00b6 Open a browser of your choice and navigate to http://localhost:9200 to verify that Elasticsearch is up and running. Make sure to check the version and the build_flavor too! Open a browser of your choice and navigate to http://localhost:5601 . You should see Kibana's starting page. Choose the \"Explore on my own\" option to get to the landing page.","title":"Exercise 2: Initializing Elasticsearch and Kibana"},{"location":"elasticsearch/exercise2/#exercise-2-initializing-elasticsearch-and-kibana","text":"The purpose of this exercise is to start Elasticsearch and Kibana . You have two options for running them: Using Docker Or install them on your machine. In the university computer laboratories use the Docker option. Otherwise choose the option best suited for you (if you don't have Docker, you can just go with option #2).","title":"Exercise 2: Initializing Elasticsearch and Kibana"},{"location":"elasticsearch/exercise2/#option-1-start-elasticsearch-and-kibana-using-docker","text":"If you have Docker, or you are working in the university computer laboratories, use this option. Start Docker Desktop and wait for it to initialize. Only in university computer laboratories : open Docker Desktop settings (right-click on the Docker icon on the taskbar), and follow the teacher's instructions to configure a proxy to be used by Docker. Open a command prompt or PowerShell console and test whether Docker is working: execute docker version , which should print version information for client and server. Locate the provided docker-compose.yml file in the checked-out starter solution (located in the root of your git repository). Open it in a text editor of your choice and check its contents. Check the following specifics: environment : This is used to set environment variables in the container. We can use these to provide some settings for Elasticsearch and Kibana as well. One example is the cluster.name variable. ulimits : In order for Elasticsearch to function properly, we have to set some ulimit values in the container. volumes : Due to how file storage works in Docker containers, every data is lost if we remove a container and start a new one. Of course, this is not exactly appropriate for a database service like Elasticsearch . Therefore we are going to use a volume which provides a more durable storage option. Navigate a command prompt / PowerShell / bash shell into the folder with the docker-compose.yml file, and issue the following commands. If you are in the university computer laboratories, please run docker-compose down -v first to remove any lingering containers and data left behind. Then to start, use docker-compose up . If you are using Linux, and this command fails, you might try to sudo to make things easier. Keep this shell open. You will see the application logs here. To shut down the running system, use CTRL + C . Your data will be kept, and restarting with the same command as above will restart the applications. To completely shut down the applications and remove all data, use docker-compose down -v .","title":"Option 1: Start Elasticsearch and Kibana using Docker"},{"location":"elasticsearch/exercise2/#option-2-start-elasticsearch-and-kibana-by-installing-them","text":"Download the OSS version of Elasticsearch and Kibana . Please make sure to use version 7.10.x ; the starter code and the instructions might be specific to this version. Extract the archives. Extracting the zip files will take a considerable amount of time. Start both using the executables bin/elasticsearch(.bat) and bin/kibana(.bat) . There is no configuration needed; the default setup will work fine Wait for a few seconds while they start. Elasticsearch will print something like this: [INFO ][o.e.h.AbstractHttpServerTransport] [xx] publish_address {127.0.0.1:9200}, bound_addresses {127.0.0.1:9200}, {[::1]:9200} [INFO ][o.e.n.Node ] [xx] started Note the port where Elasticsearch is available (should be 9200). Kibana, when ready, will print: [info][server][Kibana][http] http server running at http://localhost:5601 If you need to change the ports, edit the following files, then restart the applications: config\\elasticsearch.yml , look for key http.port , uncomment it and set a port; config\\kibana.yml , look for key elasticsearch.hosts , uncomment it and set the same port.","title":"Option 2: Start Elasticsearch and Kibana by installing them"},{"location":"elasticsearch/exercise2/#check-that-the-systems-are-running","text":"Open a browser of your choice and navigate to http://localhost:9200 to verify that Elasticsearch is up and running. Make sure to check the version and the build_flavor too! Open a browser of your choice and navigate to http://localhost:5601 . You should see Kibana's starting page. Choose the \"Explore on my own\" option to get to the landing page.","title":"Check that the systems are running"},{"location":"elasticsearch/exercise3/","text":"Exercise 3: Initializing Elasticsearch \u00b6 Now that both Elasticsearch and Kibana are operational let us create the necessary indices for us to work with in the following exercises. Index a document via PowerShell \u00b6 First, we are going to use Elasticsearch's REST API through PowerShell . To index a document in Elasticsearch , issue the following command. ( Invoke-WebRequest 'http://localhost:9200/test/_doc/1?pretty' -Method Put -ContentType 'application/json' -Body '{ \"name\": \"John Doe\" }' -UseBasicParsing ). Content This way, we inserted a document of type _doc into the index called test with id 1 . The response JSON should state \"result\": \"created\" . Query the document with the following command. ( Invoke-WebRequest 'http://localhost:9200/test/_doc/1?pretty' -Method Get -UseBasicParsing ). Content The result JSON tells us the name of the index, the id of the document, and the full document we inserted in the _source field. { \"_index\" : \"test\" , \"_type\" : \"_doc\" , \"_id\" : \"1\" , \"_version\" : 1 , \"_seq_no\" : 0 , \"_primary_term\" : 1 , \"found\" : true , \"_source\" : { \"name\" : \"John Doe\" } } Create an index and index a document using Kibana \u00b6 In this part of the exercise, we will create an index for documents containing information about people working in the fast-food industry. Here is a sample document. Sample document \u00b6 When using this sample document, make sure to replace the Neptun code with yours all uppercase in the gender and company fields. The final value should look like this: ABC123 female and ABC123 Subway respectively. { \"gender\" : \"NEPTUN female\" , \"firstName\" : \"Evelyn\" , \"lastName\" : \"Petersen\" , \"age\" : 17 , \"phone\" : \"+1 (900) 503-3892\" , \"address\" : { \"zipCode\" : 63775 , \"state\" : \"NY\" , \"city\" : \"Lynn\" , \"street\" : \"Clarkson Avenue\" , \"houseNumber\" : 503 }, \"salary\" : 87217 , \"company\" : \"NEPTUN Subway\" , \"email\" : \"evelyn.petersen@subway.com\" , \"hired\" : \"09/29/2009\" } We are going to use Kibana's Dev Tools for this part of the exercise. Although it uses the same REST API that we used through PowerShell , it provides a more convenient GUI for us to use. In this Dev tool , we can run queries. A query in Kibana's Dev Tools contains (1) a http verb and an URL matching Elasticsearch' REST API, and (2) a body as JSON. Enter PUT salaries in the first line and then enter the following JSON starting on a new line. Then press the Play button in the top right corner of the editor. { \"settings\" : { \"number_of_shards\" : 1 , \"number_of_replicas\" : 0 }, \"mappings\" : { \"properties\" : { \"gender\" : { \"type\" : \"keyword\" }, \"address.state\" : { \"type\" : \"keyword\" }, \"company\" : { \"type\" : \"keyword\" }, \"hired\" : { \"type\" : \"date\" , \"format\" : \"MM/dd/yyyy\" } } } } The settings we use here are the following. settings : We set the number of shards and replicas here. While settings the number of shards is not that important here, we must set the number of replicas to zero to have an index with green health value. This is because Elasticsearch refuses to put a shard and its replica on the same node, and we only have a single node. mapping : Mapping is the \"schema\" of the data. It is not necessary to set this, but when the data is ambiguous, Elasticsearch will make a choice how to interpret data unless we specify the mapping. gender , address.state , company : These are values we know are only going to have a few select values (e.g., \"male\" and \"female\" for gender), therefore we do not want to allow free text search on them. We can help the system by specifying this. hired : Although this is a date field, the date representation of our data is not standard \u2014 Elasticsearch wouldn't recognize it by itself. Therefore we have to specify the date format explicitly. We can check the indices with the GET _cat/indices?v query. (Just use the Dev Tools to execute this query too.) Note how the health of the test index is yellow , and the health of the salaries index is green . That is because the default value for the number of replicas is 1 . Insert the sample document into the created index. You can find the sample document here . Do not forget to edit the Neptun code in the gender field. POST salaries/_doc { ... // the sample document comes here with the correct Neptun code } Executing the query will yield a similar result (in the right side of the window). This is the response of the POST query with the id of the document inserted. We can use the _id value in the response to query the document. GET salaries/_doc/eZSmaGkBig5GeeBFsFG6 Modify the input data \u00b6 Before importing the rest of the sample data, let us make some changes by adding your Neptun code as a prefix to some of the values in the file too: Each gender value shall be prefixed, e.g. \"gender\":\"NEPTUN female\" Each company value shall be prefixed, e.g. \"company\":\"NEPTUN McDonalds\" Find the salaries.json file in the root of the repository. Open a PowerShell console here. Edit the following command by adding your Neptun code all uppercase, then execute it in PowerShell (do NOT change the quotation marks, only edit the 6 characters of the Neptun code!): ( Get-Content .\\ salaries . json ) -replace '\"gender\":\"' , '\"gender\":\"NEPTUN ' -replace '\"company\":\"' , '\"company\":\"NEPTUN ' | Set-Content .\\ salaries . json Verify the results; it should look similar (with your own Neptun code): The file must be a valid JSON! Please double-check the quotations marks around the values. If the result is not correct, you can revert the change made to this file using git ( git checkout HEAD -- salaries.json ), and then retry. The modified file shall be uploaded as part of the submission. IMPORTANT Adding your Neptun code is a mandatory step. It will be displayed on visualizations created in the following exercises. Index many documents using the bulk API \u00b6 And now, let us index these documents. We can add multiple documents to the index using the bulk API. Issue the following command from the PowerShell window in the folder of the starter solution. Invoke-WebRequest 'http://localhost:9200/_bulk' -Method Post -ContentType 'application/json' -InFile .\\ salaries . json -UseBasicParsing Check the response for errors. You will see a similar message if everything is OK (note the errors in the response): If you see a similar error, it means the source file changes resulted in an invalid json file. If this happens, you need to start over: Delete the salaries index by executing a DELETE salaries request in Kibana. Go back to the index creation step, then repeat the index creation and indexing of the single document. Reset the changes made to the salaries.json file, and retry the replacement with special care regarding the quotation marks. Now retry the bulk index request. Execute a search using query GET salaries/_search (using Kibana). This will return a few documents and let us know how many documents there are (total number matching the query will be the total number of documents, due to the lack of filtering in this search). There should be 1101 documents. If you see fewer documents, you need to use the Refresh API to ensure Elasticsearch is finished with all indexing operations. To trigger this, execute a POST salaries/_refresh request.","title":"Exercise 3: Initializing Elasticsearch"},{"location":"elasticsearch/exercise3/#exercise-3-initializing-elasticsearch","text":"Now that both Elasticsearch and Kibana are operational let us create the necessary indices for us to work with in the following exercises.","title":"Exercise 3: Initializing Elasticsearch"},{"location":"elasticsearch/exercise3/#index-a-document-via-powershell","text":"First, we are going to use Elasticsearch's REST API through PowerShell . To index a document in Elasticsearch , issue the following command. ( Invoke-WebRequest 'http://localhost:9200/test/_doc/1?pretty' -Method Put -ContentType 'application/json' -Body '{ \"name\": \"John Doe\" }' -UseBasicParsing ). Content This way, we inserted a document of type _doc into the index called test with id 1 . The response JSON should state \"result\": \"created\" . Query the document with the following command. ( Invoke-WebRequest 'http://localhost:9200/test/_doc/1?pretty' -Method Get -UseBasicParsing ). Content The result JSON tells us the name of the index, the id of the document, and the full document we inserted in the _source field. { \"_index\" : \"test\" , \"_type\" : \"_doc\" , \"_id\" : \"1\" , \"_version\" : 1 , \"_seq_no\" : 0 , \"_primary_term\" : 1 , \"found\" : true , \"_source\" : { \"name\" : \"John Doe\" } }","title":"Index a document via PowerShell"},{"location":"elasticsearch/exercise3/#create-an-index-and-index-a-document-using-kibana","text":"In this part of the exercise, we will create an index for documents containing information about people working in the fast-food industry. Here is a sample document.","title":"Create an index and index a document using Kibana"},{"location":"elasticsearch/exercise3/#sample-document","text":"When using this sample document, make sure to replace the Neptun code with yours all uppercase in the gender and company fields. The final value should look like this: ABC123 female and ABC123 Subway respectively. { \"gender\" : \"NEPTUN female\" , \"firstName\" : \"Evelyn\" , \"lastName\" : \"Petersen\" , \"age\" : 17 , \"phone\" : \"+1 (900) 503-3892\" , \"address\" : { \"zipCode\" : 63775 , \"state\" : \"NY\" , \"city\" : \"Lynn\" , \"street\" : \"Clarkson Avenue\" , \"houseNumber\" : 503 }, \"salary\" : 87217 , \"company\" : \"NEPTUN Subway\" , \"email\" : \"evelyn.petersen@subway.com\" , \"hired\" : \"09/29/2009\" } We are going to use Kibana's Dev Tools for this part of the exercise. Although it uses the same REST API that we used through PowerShell , it provides a more convenient GUI for us to use. In this Dev tool , we can run queries. A query in Kibana's Dev Tools contains (1) a http verb and an URL matching Elasticsearch' REST API, and (2) a body as JSON. Enter PUT salaries in the first line and then enter the following JSON starting on a new line. Then press the Play button in the top right corner of the editor. { \"settings\" : { \"number_of_shards\" : 1 , \"number_of_replicas\" : 0 }, \"mappings\" : { \"properties\" : { \"gender\" : { \"type\" : \"keyword\" }, \"address.state\" : { \"type\" : \"keyword\" }, \"company\" : { \"type\" : \"keyword\" }, \"hired\" : { \"type\" : \"date\" , \"format\" : \"MM/dd/yyyy\" } } } } The settings we use here are the following. settings : We set the number of shards and replicas here. While settings the number of shards is not that important here, we must set the number of replicas to zero to have an index with green health value. This is because Elasticsearch refuses to put a shard and its replica on the same node, and we only have a single node. mapping : Mapping is the \"schema\" of the data. It is not necessary to set this, but when the data is ambiguous, Elasticsearch will make a choice how to interpret data unless we specify the mapping. gender , address.state , company : These are values we know are only going to have a few select values (e.g., \"male\" and \"female\" for gender), therefore we do not want to allow free text search on them. We can help the system by specifying this. hired : Although this is a date field, the date representation of our data is not standard \u2014 Elasticsearch wouldn't recognize it by itself. Therefore we have to specify the date format explicitly. We can check the indices with the GET _cat/indices?v query. (Just use the Dev Tools to execute this query too.) Note how the health of the test index is yellow , and the health of the salaries index is green . That is because the default value for the number of replicas is 1 . Insert the sample document into the created index. You can find the sample document here . Do not forget to edit the Neptun code in the gender field. POST salaries/_doc { ... // the sample document comes here with the correct Neptun code } Executing the query will yield a similar result (in the right side of the window). This is the response of the POST query with the id of the document inserted. We can use the _id value in the response to query the document. GET salaries/_doc/eZSmaGkBig5GeeBFsFG6","title":"Sample document"},{"location":"elasticsearch/exercise3/#modify-the-input-data","text":"Before importing the rest of the sample data, let us make some changes by adding your Neptun code as a prefix to some of the values in the file too: Each gender value shall be prefixed, e.g. \"gender\":\"NEPTUN female\" Each company value shall be prefixed, e.g. \"company\":\"NEPTUN McDonalds\" Find the salaries.json file in the root of the repository. Open a PowerShell console here. Edit the following command by adding your Neptun code all uppercase, then execute it in PowerShell (do NOT change the quotation marks, only edit the 6 characters of the Neptun code!): ( Get-Content .\\ salaries . json ) -replace '\"gender\":\"' , '\"gender\":\"NEPTUN ' -replace '\"company\":\"' , '\"company\":\"NEPTUN ' | Set-Content .\\ salaries . json Verify the results; it should look similar (with your own Neptun code): The file must be a valid JSON! Please double-check the quotations marks around the values. If the result is not correct, you can revert the change made to this file using git ( git checkout HEAD -- salaries.json ), and then retry. The modified file shall be uploaded as part of the submission. IMPORTANT Adding your Neptun code is a mandatory step. It will be displayed on visualizations created in the following exercises.","title":"Modify the input data"},{"location":"elasticsearch/exercise3/#index-many-documents-using-the-bulk-api","text":"And now, let us index these documents. We can add multiple documents to the index using the bulk API. Issue the following command from the PowerShell window in the folder of the starter solution. Invoke-WebRequest 'http://localhost:9200/_bulk' -Method Post -ContentType 'application/json' -InFile .\\ salaries . json -UseBasicParsing Check the response for errors. You will see a similar message if everything is OK (note the errors in the response): If you see a similar error, it means the source file changes resulted in an invalid json file. If this happens, you need to start over: Delete the salaries index by executing a DELETE salaries request in Kibana. Go back to the index creation step, then repeat the index creation and indexing of the single document. Reset the changes made to the salaries.json file, and retry the replacement with special care regarding the quotation marks. Now retry the bulk index request. Execute a search using query GET salaries/_search (using Kibana). This will return a few documents and let us know how many documents there are (total number matching the query will be the total number of documents, due to the lack of filtering in this search). There should be 1101 documents. If you see fewer documents, you need to use the Refresh API to ensure Elasticsearch is finished with all indexing operations. To trigger this, execute a POST salaries/_refresh request.","title":"Index many documents using the bulk API"},{"location":"elasticsearch/exercise4/","text":"Exercise 4: Elasticsearch queries \u00b6 The purpose of this exercise is to use the search and aggregation capabilities of Elasticsearch to answer questions of the data imported previously. Query JSON You will be requested to save the query JSON in the following exercises. The image below shows you what that means (i.e., the VALID JSON sent to Elasticsearch, WITHOUT the header part; NOT the result JSON that is on the right side in Kibana). Search syntax Please use the search syntax of Elasticsearch that defines the criteria in JSON body. Do not use the URL query method filtering. All of the following queries are sent to the salaries/_search endpoint with a GET method. This means that the first line in Kibana Dev Tools will be GET salaries/_search for all of them. \"SELECT *\" in Elasticsearch \u00b6 Issue the following query. { \"query\" : { \"match_all\" : {} }, \"from\" : 0 , \"size\" : 10 , \"sort\" : [ \"_doc\" ] } query : This provides the filter for the query. Think of the WHERE clause in SQL . from and size : These can be used for paging results. It is important to note that there is no way to query all documents using Elasticsearch . If you omit the size value, it defaults to 10 . sort : This can be used to sort the results. You do not have to save this query. a) Who are the top 5 workers with the best salaries? \u00b6 Change the previous query: we need the first 5 , so the size will be 5 ; the sort has to be performed on field salary , for which use the following syntax: { ... \"sort\" : [{ \"salary\" : { \"order\" : \"desc\" } }] } Execute this query and verify the results. SUBMISSION Save the final query JSON as ex4-a.json . b) Who are the top 5 workers at McDonalds aged between 18 and 30 with the best salaries? \u00b6 Building on the previous query, replace the query with a boolean query as follows: { \"query\" : { \"bool\" : { \"must\" : { \"match_all\" : {} }, \"filter\" : [ { \"range\" : { \"age\" : { \"gte\" : 18 , \"lte\" : 30 } } }, { \"term\" : { \"company\" : \"NEPTUN McDonalds\" } } ] } }, ... } The difference between this query and the previous one is that we have to apply some filters to the results. The age must be between 18 and 30 . The company must be NEPTUN McDonalds prefixed with your Neptun code. Execute this query and verify the results. SUBMISSION Save the final query JSON as ex4-b.json . c) Are there more men or women working for these companies? Is there a difference between the average salaries? \u00b6 We have to use term aggregations to answer these questions. We can use the following query. { \"size\" : 0 , \"aggs\" : { \"group_by_gender\" : { \"terms\" : { \"field\" : \"gender\" }, \"aggs\" : { \"average_salary\" : { \"avg\" : { \"field\" : \"salary\" } } } } } } Specifying \"size\": 0 means that we do not want any matching documents as we only need the aggregation results. (There are no search criteria specified here, but there could be.) This query first groups the documents by the gender value and then calculates the average of the salary values within the groups. SUBMISSION Based on the results, what is the average salary of women? Type your answer into ex4-c.txt . Include only the number and nothing else in this text file (e.g., 123.45 ). (You do not need to save the query JSON here.)","title":"Exercise 4: Elasticsearch queries"},{"location":"elasticsearch/exercise4/#exercise-4-elasticsearch-queries","text":"The purpose of this exercise is to use the search and aggregation capabilities of Elasticsearch to answer questions of the data imported previously. Query JSON You will be requested to save the query JSON in the following exercises. The image below shows you what that means (i.e., the VALID JSON sent to Elasticsearch, WITHOUT the header part; NOT the result JSON that is on the right side in Kibana). Search syntax Please use the search syntax of Elasticsearch that defines the criteria in JSON body. Do not use the URL query method filtering. All of the following queries are sent to the salaries/_search endpoint with a GET method. This means that the first line in Kibana Dev Tools will be GET salaries/_search for all of them.","title":"Exercise 4: Elasticsearch queries"},{"location":"elasticsearch/exercise4/#select-in-elasticsearch","text":"Issue the following query. { \"query\" : { \"match_all\" : {} }, \"from\" : 0 , \"size\" : 10 , \"sort\" : [ \"_doc\" ] } query : This provides the filter for the query. Think of the WHERE clause in SQL . from and size : These can be used for paging results. It is important to note that there is no way to query all documents using Elasticsearch . If you omit the size value, it defaults to 10 . sort : This can be used to sort the results. You do not have to save this query.","title":"\"SELECT *\" in Elasticsearch"},{"location":"elasticsearch/exercise4/#a-who-are-the-top-5-workers-with-the-best-salaries","text":"Change the previous query: we need the first 5 , so the size will be 5 ; the sort has to be performed on field salary , for which use the following syntax: { ... \"sort\" : [{ \"salary\" : { \"order\" : \"desc\" } }] } Execute this query and verify the results. SUBMISSION Save the final query JSON as ex4-a.json .","title":"a) Who are the top 5 workers with the best salaries?"},{"location":"elasticsearch/exercise4/#b-who-are-the-top-5-workers-at-mcdonalds-aged-between-18-and-30-with-the-best-salaries","text":"Building on the previous query, replace the query with a boolean query as follows: { \"query\" : { \"bool\" : { \"must\" : { \"match_all\" : {} }, \"filter\" : [ { \"range\" : { \"age\" : { \"gte\" : 18 , \"lte\" : 30 } } }, { \"term\" : { \"company\" : \"NEPTUN McDonalds\" } } ] } }, ... } The difference between this query and the previous one is that we have to apply some filters to the results. The age must be between 18 and 30 . The company must be NEPTUN McDonalds prefixed with your Neptun code. Execute this query and verify the results. SUBMISSION Save the final query JSON as ex4-b.json .","title":"b) Who are the top 5 workers at McDonalds aged between 18 and 30 with the best salaries?"},{"location":"elasticsearch/exercise4/#c-are-there-more-men-or-women-working-for-these-companies-is-there-a-difference-between-the-average-salaries","text":"We have to use term aggregations to answer these questions. We can use the following query. { \"size\" : 0 , \"aggs\" : { \"group_by_gender\" : { \"terms\" : { \"field\" : \"gender\" }, \"aggs\" : { \"average_salary\" : { \"avg\" : { \"field\" : \"salary\" } } } } } } Specifying \"size\": 0 means that we do not want any matching documents as we only need the aggregation results. (There are no search criteria specified here, but there could be.) This query first groups the documents by the gender value and then calculates the average of the salary values within the groups. SUBMISSION Based on the results, what is the average salary of women? Type your answer into ex4-c.txt . Include only the number and nothing else in this text file (e.g., 123.45 ). (You do not need to save the query JSON here.)","title":"c) Are there more men or women working for these companies? Is there a difference between the average salaries?"},{"location":"elasticsearch/exercise5/","text":"Exercise 5: Kibana visualizations \u00b6 The purpose of this exercise is to use the data visualization capabilities of Kibana . Screenshots The following exercises will ask you to create visualizations, then export their Kibana description as JSON, and create a screenshot of the visualization itself. When creating the screenshot, make sure the entire visualization with the legend is visible. Create an Index pattern \u00b6 Our first step is to tell Kibana which indexes it should consider when creating the visualizations. Click on the Visualize tab on the left side menu. When opening this page the first time, you will be redirected to the Management / Index patterns configuration page to create a new index pattern . Create a new index pattern. Enter the index name \u2014 salaries \u2014 as the index pattern. Make sure Kibana says Success! Your pattern matches 1 index , and click Next step . Select I don't want to use the Time Filter as Time Filter field name since we will not use this function during the exercises. Click on Create index pattern . Now click on the Visualize tab again to see the following. a) How many people did KFC hire each month? (vertical bar chart) \u00b6 Click on the Create new visualization button on the Visualize tab, and select Vertical Bar . Select the previously created salaries index pattern as the search source. Under the Metrics setting, set People hired as the Custom Label . (Expand the Y-axis label to get the configuration options.) Under the Buckets setting, click Add , then select X-Axis , and set the following. Aggregation should be Date Histogram Field should be hired Minimum interval should be Month Custom Label should be Month . In the top left corner, click on the Add a filter link and filter for company \"KFC\" (with the appropriate prefix): use the following settings to create the filter. Click on Save to save the filter. The configuration of the visualization is now ready. You see the preview of the visualization. Click on the Save button in the top right corner to save the visualization as 5_a . SUBMISSION Create a screenshot of the visualization preview and save it as ex5-a.png . Make sure that the filter, including your Neptun code, is visible on this screenshot. Click on the Stack Management tab in the left side menu, and choose the Saved Objects option. Select and export the visualization you just saved \u2014 no need to include related objects. Save the downloaded file as ex5-a.ndjson . b) Show the gender and age distribution of the workers! (pie chart) \u00b6 Go back to the Visualize tab. It will likely load the last visualization. Click on the tab button again to get back to the landing page of all visualizations. Click on the Create visualization button on the Visualize tab, and select Pie . Select the previously created salaries index pattern as the search source. Under the Buckets setting, click Add and select Split Slices and set a Terms aggregation on the gender field. As Custom Label set Gender . Within the Buckets configuration area, click on the Add button again and select Split Slices with a Range aggregation on the age field with the following ranges. Also, add a custom label Age . SUBMISSION Create a screenshot of the resulting visualization and save it as ex5-b.png . Use the previous method to save and export the visualization. Save the exported file as ex5-b.ndjson . c) Show the distribution of the workers' locations on a map! \u00b6 Create a new visualization of type Region Map . Select the previously created salaries index pattern as the search source. Under the Buckets setting add a Shape field with Terms aggregation on the address.state field. Make sure to set the Size value to at least 50. Under Options / Layer Settings select USA States as Vector map and FIPS 5-2 alpha code as Join field . SUBMISSION Create a screenshot of the result and save it as ex5-c.png . Save and export the visualization. Save the exported file as ex5-c.ndjson .","title":"Exercise 5: Kibana visualizations"},{"location":"elasticsearch/exercise5/#exercise-5-kibana-visualizations","text":"The purpose of this exercise is to use the data visualization capabilities of Kibana . Screenshots The following exercises will ask you to create visualizations, then export their Kibana description as JSON, and create a screenshot of the visualization itself. When creating the screenshot, make sure the entire visualization with the legend is visible.","title":"Exercise 5: Kibana visualizations"},{"location":"elasticsearch/exercise5/#create-an-index-pattern","text":"Our first step is to tell Kibana which indexes it should consider when creating the visualizations. Click on the Visualize tab on the left side menu. When opening this page the first time, you will be redirected to the Management / Index patterns configuration page to create a new index pattern . Create a new index pattern. Enter the index name \u2014 salaries \u2014 as the index pattern. Make sure Kibana says Success! Your pattern matches 1 index , and click Next step . Select I don't want to use the Time Filter as Time Filter field name since we will not use this function during the exercises. Click on Create index pattern . Now click on the Visualize tab again to see the following.","title":"Create an Index pattern"},{"location":"elasticsearch/exercise5/#a-how-many-people-did-kfc-hire-each-month-vertical-bar-chart","text":"Click on the Create new visualization button on the Visualize tab, and select Vertical Bar . Select the previously created salaries index pattern as the search source. Under the Metrics setting, set People hired as the Custom Label . (Expand the Y-axis label to get the configuration options.) Under the Buckets setting, click Add , then select X-Axis , and set the following. Aggregation should be Date Histogram Field should be hired Minimum interval should be Month Custom Label should be Month . In the top left corner, click on the Add a filter link and filter for company \"KFC\" (with the appropriate prefix): use the following settings to create the filter. Click on Save to save the filter. The configuration of the visualization is now ready. You see the preview of the visualization. Click on the Save button in the top right corner to save the visualization as 5_a . SUBMISSION Create a screenshot of the visualization preview and save it as ex5-a.png . Make sure that the filter, including your Neptun code, is visible on this screenshot. Click on the Stack Management tab in the left side menu, and choose the Saved Objects option. Select and export the visualization you just saved \u2014 no need to include related objects. Save the downloaded file as ex5-a.ndjson .","title":"a) How many people did KFC hire each month? (vertical bar chart)"},{"location":"elasticsearch/exercise5/#b-show-the-gender-and-age-distribution-of-the-workers-pie-chart","text":"Go back to the Visualize tab. It will likely load the last visualization. Click on the tab button again to get back to the landing page of all visualizations. Click on the Create visualization button on the Visualize tab, and select Pie . Select the previously created salaries index pattern as the search source. Under the Buckets setting, click Add and select Split Slices and set a Terms aggregation on the gender field. As Custom Label set Gender . Within the Buckets configuration area, click on the Add button again and select Split Slices with a Range aggregation on the age field with the following ranges. Also, add a custom label Age . SUBMISSION Create a screenshot of the resulting visualization and save it as ex5-b.png . Use the previous method to save and export the visualization. Save the exported file as ex5-b.ndjson .","title":"b) Show the gender and age distribution of the workers! (pie chart)"},{"location":"elasticsearch/exercise5/#c-show-the-distribution-of-the-workers-locations-on-a-map","text":"Create a new visualization of type Region Map . Select the previously created salaries index pattern as the search source. Under the Buckets setting add a Shape field with Terms aggregation on the address.state field. Make sure to set the Size value to at least 50. Under Options / Layer Settings select USA States as Vector map and FIPS 5-2 alpha code as Join field . SUBMISSION Create a screenshot of the result and save it as ex5-c.png . Save and export the visualization. Save the exported file as ex5-c.ndjson .","title":"c) Show the distribution of the workers' locations on a map!"},{"location":"elasticsearch/exercise6/","text":"Exercise 6: Elasticsearch queries \u2014 practice \u00b6 The purpose of this exercise is to practice the search and aggregation capabilities of Elasticsearch . Feel free to take inspiration from the solutions in exercise 4 . Please use the Dev Tools of Kibana for creating and testing of the queries. a) Who are the 3 youngest people? b) Who are the 3 youngest people working at KFC earning at least 80000 USD per year? c) What is the average age of the workers at different companies? SUBMISSION As in exercise 4, you are required to save the query JSON as part of this exercise. Please refer to exercise 4 for an explanation of what the query JSON is. Save the query JSON as ex6-a.json , ex6-b.json , and ex6-c.json respectively.","title":"Exercise 6: Elasticsearch queries \u2014 practice"},{"location":"elasticsearch/exercise6/#exercise-6-elasticsearch-queries-practice","text":"The purpose of this exercise is to practice the search and aggregation capabilities of Elasticsearch . Feel free to take inspiration from the solutions in exercise 4 . Please use the Dev Tools of Kibana for creating and testing of the queries. a) Who are the 3 youngest people? b) Who are the 3 youngest people working at KFC earning at least 80000 USD per year? c) What is the average age of the workers at different companies? SUBMISSION As in exercise 4, you are required to save the query JSON as part of this exercise. Please refer to exercise 4 for an explanation of what the query JSON is. Save the query JSON as ex6-a.json , ex6-b.json , and ex6-c.json respectively.","title":"Exercise 6: Elasticsearch queries \u2014 practice"},{"location":"elasticsearch/exercise7/","text":"Exercise 7: Kibana visualizations \u2014 practice \u00b6 The purpose of this exercise is to practice the data visualization capabilities of Kibana . Feel free to take inspiration from the solutions in exercise 5 . Create three visualizations to answer the following questions \u00b6 a) What was the average age of the people hired each year in the different companies? Use a vertical bar chart for visualization. Tips You will have to use two aggregations to create this visualization. By default, Kibana stacks the data for the different companies on top of each other. For our use-case, it would be much better if these would be next to each other instead. Fortunately, you can set this if you choose the Mode called normal instead of stacked under the Metrics & Axes settings. Make sure to set the Metrics to Average of age instead of Count ! b) What is the distribution of the workers between the various companies in the state of New York (NY)? Use a pie chart for visualization. c) What is the average salary of the workers aged between 18 and 30 in the various states? Make sure to set the Metrics to Average of salary instead of Count ! Use a region map for visualization. Your visualizations, when finished and added to the dashboard (next part of the exercise) should look similar to this: Create a dashboard \u00b6 When all three visualizations are ready, create a new dashboard, and add the visualizations onto this dashboard. Click on the Dashboard item on the left side menu. Create a new dashboard here. Use the Add button on the dashboard to add existing visualizations. Add the three you created in this exercise. You can drag and drop them on the dashboard to various places, change their size, etc. Save the dashboard (just like you saved each visualization). SUBMISSION Create a screenshot of this dashboard and save it as ex7.png . The screenshot should show the entire dashboard with all three visualizations visible. Export the dashboard ndjson file as ex7.ndjson , as done previously, but this time also include related objects (this is a setting in Kibana when exporting).","title":"Exercise 7: Kibana visualizations \u2014 practice"},{"location":"elasticsearch/exercise7/#exercise-7-kibana-visualizations-practice","text":"The purpose of this exercise is to practice the data visualization capabilities of Kibana . Feel free to take inspiration from the solutions in exercise 5 .","title":"Exercise 7: Kibana visualizations \u2014 practice"},{"location":"elasticsearch/exercise7/#create-three-visualizations-to-answer-the-following-questions","text":"a) What was the average age of the people hired each year in the different companies? Use a vertical bar chart for visualization. Tips You will have to use two aggregations to create this visualization. By default, Kibana stacks the data for the different companies on top of each other. For our use-case, it would be much better if these would be next to each other instead. Fortunately, you can set this if you choose the Mode called normal instead of stacked under the Metrics & Axes settings. Make sure to set the Metrics to Average of age instead of Count ! b) What is the distribution of the workers between the various companies in the state of New York (NY)? Use a pie chart for visualization. c) What is the average salary of the workers aged between 18 and 30 in the various states? Make sure to set the Metrics to Average of salary instead of Count ! Use a region map for visualization. Your visualizations, when finished and added to the dashboard (next part of the exercise) should look similar to this:","title":"Create three visualizations to answer the following questions"},{"location":"elasticsearch/exercise7/#create-a-dashboard","text":"When all three visualizations are ready, create a new dashboard, and add the visualizations onto this dashboard. Click on the Dashboard item on the left side menu. Create a new dashboard here. Use the Add button on the dashboard to add existing visualizations. Add the three you created in this exercise. You can drag and drop them on the dashboard to various places, change their size, etc. Save the dashboard (just like you saved each visualization). SUBMISSION Create a screenshot of this dashboard and save it as ex7.png . The screenshot should show the entire dashboard with all three visualizations visible. Export the dashboard ndjson file as ex7.ndjson , as done previously, but this time also include related objects (this is a setting in Kibana when exporting).","title":"Create a dashboard"},{"location":"mssql/","text":"Microsoft SQL Server Business Intelligence \u00b6 This laboratory's goal is to practice the usage of Microsoft SQL Server products in business intelligence scenarios. Through the exercises, you will familiarize yourself with the usage of Microsoft SQL Server as a data warehouse, Integration Services as an ETL tool, and Reporting Services for report generation. Pre-requisites \u00b6 You need to following tools to complete this laboratory: PC with Windows. You can also use a VM at https://cloud.bme.hu , see details here . Microsoft SQL Server. Express edition is sufficient, or localdb installed along with Visual Studio is also fine SQL Server Management Studio Microsoft Visual Studio 2019. The community version is sufficient. Integration Services and Report Server Project support for Visual Studio Microsoft Reporting Services Projects extension SQL Server Integration Services Projects extension . It is recommended that these extensions are kept up to date . A GitHub account and a git tool Material to review before the laboratory \u00b6 The expected mode of submitting your work, as detailed here . The material covered in course Business intelligence related to the topic, including, but not limited to The demo material covered during the semester ( https://github.com/peekler/Business-Intelligence-Demos/tree/master/MSSQL-BI ) The short introduction to using these software. This guide provides solutions to a few typical issues too, feel free to refer to this guide. SQL Integration Services tutorial SQL Reporting Services tutorial Task overview \u00b6 We have a webshop that sells books. Our goal is to understand our users and find out which are the best books we should market more. We have the users of the webshop, the books, and user-given ratings of these books. We already exported these data in the form of CSV files. We will use Microsoft SQL Server as a data warehouse, Integration Services as the ETL tool, and Reporting Services to present our insights in a visual form. The model of our approach looks like the following. Grading \u00b6 Exercises 1 & 2 help you prepare the environment. Starting with exercise 3, each exercise is worth one grade when completed successfully . For an exercise to count as successful you must finish all its subtasks successfully. (E.g. if you complete exercises 3, 4 successfully, but there is an error in exercise 5, and you did not complete exercise 6, the resulting grade will be 3.)","title":"Microsoft SQL Server Business Intelligence"},{"location":"mssql/#microsoft-sql-server-business-intelligence","text":"This laboratory's goal is to practice the usage of Microsoft SQL Server products in business intelligence scenarios. Through the exercises, you will familiarize yourself with the usage of Microsoft SQL Server as a data warehouse, Integration Services as an ETL tool, and Reporting Services for report generation.","title":"Microsoft SQL Server Business Intelligence"},{"location":"mssql/#pre-requisites","text":"You need to following tools to complete this laboratory: PC with Windows. You can also use a VM at https://cloud.bme.hu , see details here . Microsoft SQL Server. Express edition is sufficient, or localdb installed along with Visual Studio is also fine SQL Server Management Studio Microsoft Visual Studio 2019. The community version is sufficient. Integration Services and Report Server Project support for Visual Studio Microsoft Reporting Services Projects extension SQL Server Integration Services Projects extension . It is recommended that these extensions are kept up to date . A GitHub account and a git tool","title":"Pre-requisites"},{"location":"mssql/#material-to-review-before-the-laboratory","text":"The expected mode of submitting your work, as detailed here . The material covered in course Business intelligence related to the topic, including, but not limited to The demo material covered during the semester ( https://github.com/peekler/Business-Intelligence-Demos/tree/master/MSSQL-BI ) The short introduction to using these software. This guide provides solutions to a few typical issues too, feel free to refer to this guide. SQL Integration Services tutorial SQL Reporting Services tutorial","title":"Material to review before the laboratory"},{"location":"mssql/#task-overview","text":"We have a webshop that sells books. Our goal is to understand our users and find out which are the best books we should market more. We have the users of the webshop, the books, and user-given ratings of these books. We already exported these data in the form of CSV files. We will use Microsoft SQL Server as a data warehouse, Integration Services as the ETL tool, and Reporting Services to present our insights in a visual form. The model of our approach looks like the following.","title":"Task overview"},{"location":"mssql/#grading","text":"Exercises 1 & 2 help you prepare the environment. Starting with exercise 3, each exercise is worth one grade when completed successfully . For an exercise to count as successful you must finish all its subtasks successfully. (E.g. if you complete exercises 3, 4 successfully, but there is an error in exercise 5, and you did not complete exercise 6, the resulting grade will be 3.)","title":"Grading"},{"location":"mssql/bi-software-intro/","text":"Using MSSQL BI tools \u00b6 This guide provides a quick introduction using Microsoft SQL Server Business Intelligence tools. SQL Server Management Studio \u00b6 If you need to create a new database, or create the tables, use SQL Server Management Studio . You need to connect to a Database engine and use the server name and authentication method detailed in the course material. After a successful connection, the databases are listed in the Object Explorer . Right-click the Databases folder to create a new database. To run a new query, click the New query button, write the query, and click Execute . Typical issues \u00b6 I just created a new database/table, but it does not appear in SSMS Refresh the list by right-clicking the Databases or Tables folder in the Object explorer . Microsoft SQL Server Data Tools integrated into Visual Studio \u00b6 After installing Microsoft SQL Server Data Tools, you will find new project types in Visual Studio in the New Project wizard. The projects themselves contain mostly xml-based descriptions. For production use, these so-called packages are published to servers for execution. For debugging purposes, Visual Studio is also able to execute the packages. Integration Services project \u00b6 The Integration services project contains a Package.dtsx (visible in Solution Explorer), which, when opened, presents a designer UI. There are two views to this UI: the Control Flow and the Data Flow . Control flow is the high-level component responsible for the execution of the entire process; Data flow contains part of the whole process, a single ETL flow. We build ETL processes by combining the two. The ETL process is built from the available components by dragging them from the SSIS toolbox and connecting them. The SSIS toolbox can be opened from the View menu under Other Windows . Each component's settings can be edited in the Properties box on the right, or by double clicking the boxes themselves. Notable components (from the SSIS Toolbox ) that you will need: Data Flow Task: data extract-transform-load sequence of steps Execute SQL Task: execute a command ADO NET/ Excel / ODBC/ Flat File / etc. Source/Destination: load or save data to/from Derive Column: transform a value or calculate a new value from existing data Conditional Split: split the data into two paths based on a condition Sort: sorting and duplicate filtering We can debug the process using the Debug button in the toolbar. During the execution, we will get immediate feedback on the progress and the result. The detailed log can be viewed by clicking the Progress button, or in the Output window . If there are errors during execution, you will see the component with issues marked with a red circle. In most cases, the actual error you can find in the Output window. Typical issues \u00b6 The integration services project failed to execute Check the log in the Progress view, and locate the first error. You changed the properties of a connection manager and \"nothing works\" You need to open the data input or output component by double-clicking it. It will update the changes made to the connection manager. CSV import fails if you removed columns in the flat file source connection manager Do not remove columns in the file source. You will just ignore the unnecessary columns later on. \"Truncation\" related errors You specified incorrect length in the flat file source connection details. \"... failed validation and returned validation status VS_NEEDSNEWMETADATA\", \"... has been out of synchronization with the database column\", or similar errors You probably changed a column length either in the flat file source or in the database, and the ETL process definition is not aware of this change. You need to find the component with this error (the name is in the error message), then right-click it and open the Advanced editor and click Refresh . If doing this for a single component does not solve the problem, repeat the same action for all components, starting with the input source and following the data flow direction (this synchronizes the input of each component with the output of the previous one, hence the need to do this in order). When running the IS process in Visual studio it complains about \"file being used\" Close Visual Studio. Start the Task Manager, and kill the \"IS Debug Host\" processes. Re-open the solution in Visual Studio and try again. Reporting Services project \u00b6 The easiest way to start with a new project is to use the Reporting Server Project Wizard . It guides you through creating the project and then a new report. The report is presented in a WYSIWYG manner in a designer surface. You can add new elements from the Toolbox (open the toolbox from View menu under Other Windows ). The layout and contents are edited here, while the Properties of each element is specified in the toolbox on the right. You can also Preview the report here. Every report has a Data source , which is the connection to the database. You need to create this manually, or if you used the wizard to create the project, it might already exist. A Data set specifies the query that will fetch the data the report displays. Typically you create one data set for each component (table, diagram, etc.) you add to the report. The result of this query can be bound to fields of the report, e.g., display it in one column of the table. You can find these in the Report Data window (open it from the View menu). A report project can contain multiple reports. You can add new ones using the Solution Explorer. Typical issues \u00b6 There was no endpoint listening at net.pipe Close Visual Studio then start it as administrator.","title":"Using MSSQL BI tools"},{"location":"mssql/bi-software-intro/#using-mssql-bi-tools","text":"This guide provides a quick introduction using Microsoft SQL Server Business Intelligence tools.","title":"Using MSSQL BI tools"},{"location":"mssql/bi-software-intro/#sql-server-management-studio","text":"If you need to create a new database, or create the tables, use SQL Server Management Studio . You need to connect to a Database engine and use the server name and authentication method detailed in the course material. After a successful connection, the databases are listed in the Object Explorer . Right-click the Databases folder to create a new database. To run a new query, click the New query button, write the query, and click Execute .","title":"SQL Server Management Studio"},{"location":"mssql/bi-software-intro/#typical-issues","text":"I just created a new database/table, but it does not appear in SSMS Refresh the list by right-clicking the Databases or Tables folder in the Object explorer .","title":"Typical issues"},{"location":"mssql/bi-software-intro/#microsoft-sql-server-data-tools-integrated-into-visual-studio","text":"After installing Microsoft SQL Server Data Tools, you will find new project types in Visual Studio in the New Project wizard. The projects themselves contain mostly xml-based descriptions. For production use, these so-called packages are published to servers for execution. For debugging purposes, Visual Studio is also able to execute the packages.","title":"Microsoft SQL Server Data Tools integrated into Visual Studio"},{"location":"mssql/bi-software-intro/#integration-services-project","text":"The Integration services project contains a Package.dtsx (visible in Solution Explorer), which, when opened, presents a designer UI. There are two views to this UI: the Control Flow and the Data Flow . Control flow is the high-level component responsible for the execution of the entire process; Data flow contains part of the whole process, a single ETL flow. We build ETL processes by combining the two. The ETL process is built from the available components by dragging them from the SSIS toolbox and connecting them. The SSIS toolbox can be opened from the View menu under Other Windows . Each component's settings can be edited in the Properties box on the right, or by double clicking the boxes themselves. Notable components (from the SSIS Toolbox ) that you will need: Data Flow Task: data extract-transform-load sequence of steps Execute SQL Task: execute a command ADO NET/ Excel / ODBC/ Flat File / etc. Source/Destination: load or save data to/from Derive Column: transform a value or calculate a new value from existing data Conditional Split: split the data into two paths based on a condition Sort: sorting and duplicate filtering We can debug the process using the Debug button in the toolbar. During the execution, we will get immediate feedback on the progress and the result. The detailed log can be viewed by clicking the Progress button, or in the Output window . If there are errors during execution, you will see the component with issues marked with a red circle. In most cases, the actual error you can find in the Output window.","title":"Integration Services project"},{"location":"mssql/bi-software-intro/#typical-issues_1","text":"The integration services project failed to execute Check the log in the Progress view, and locate the first error. You changed the properties of a connection manager and \"nothing works\" You need to open the data input or output component by double-clicking it. It will update the changes made to the connection manager. CSV import fails if you removed columns in the flat file source connection manager Do not remove columns in the file source. You will just ignore the unnecessary columns later on. \"Truncation\" related errors You specified incorrect length in the flat file source connection details. \"... failed validation and returned validation status VS_NEEDSNEWMETADATA\", \"... has been out of synchronization with the database column\", or similar errors You probably changed a column length either in the flat file source or in the database, and the ETL process definition is not aware of this change. You need to find the component with this error (the name is in the error message), then right-click it and open the Advanced editor and click Refresh . If doing this for a single component does not solve the problem, repeat the same action for all components, starting with the input source and following the data flow direction (this synchronizes the input of each component with the output of the previous one, hence the need to do this in order). When running the IS process in Visual studio it complains about \"file being used\" Close Visual Studio. Start the Task Manager, and kill the \"IS Debug Host\" processes. Re-open the solution in Visual Studio and try again.","title":"Typical issues"},{"location":"mssql/bi-software-intro/#reporting-services-project","text":"The easiest way to start with a new project is to use the Reporting Server Project Wizard . It guides you through creating the project and then a new report. The report is presented in a WYSIWYG manner in a designer surface. You can add new elements from the Toolbox (open the toolbox from View menu under Other Windows ). The layout and contents are edited here, while the Properties of each element is specified in the toolbox on the right. You can also Preview the report here. Every report has a Data source , which is the connection to the database. You need to create this manually, or if you used the wizard to create the project, it might already exist. A Data set specifies the query that will fetch the data the report displays. Typically you create one data set for each component (table, diagram, etc.) you add to the report. The result of this query can be bound to fields of the report, e.g., display it in one column of the table. You can find these in the Report Data window (open it from the View menu). A report project can contain multiple reports. You can add new ones using the Solution Explorer.","title":"Reporting Services project"},{"location":"mssql/bi-software-intro/#typical-issues_2","text":"There was no endpoint listening at net.pipe Close Visual Studio then start it as administrator.","title":"Typical issues"},{"location":"mssql/bme-cloud-vm-usage/","text":"Using BME Cloud \u00b6 Go to https://cloud.bme.hu and log in with your EduID. A prepared VM template is available in the \"Smallville\" data center with the name \"BI laboratory MSSQL.\" You can use this template to create a VM that has all the necessary software installed. You will need an RDP client to connect to the cloud VM. Windows has the built-in Remote Desktop Connection software, on a Mac, you can use Microsoft Remote Desktop , and on Linux Remmina - or any other RDP-compatible software. Please note, that although being installed already, you may need to log into Visual Studio with a Microsoft account (e.g., your @edu.bme.hu account). VM performance Please note that the performance of the VM depends on the usage of the cloud. You may experience a slowdown compared to a performant machine. The exercises can be solved on the VM in due time (we have tested it). You can help the VM but not running multiple Visual Studio instances at the same time and not opening the browser inside the VM.","title":"Using BME Cloud"},{"location":"mssql/bme-cloud-vm-usage/#using-bme-cloud","text":"Go to https://cloud.bme.hu and log in with your EduID. A prepared VM template is available in the \"Smallville\" data center with the name \"BI laboratory MSSQL.\" You can use this template to create a VM that has all the necessary software installed. You will need an RDP client to connect to the cloud VM. Windows has the built-in Remote Desktop Connection software, on a Mac, you can use Microsoft Remote Desktop , and on Linux Remmina - or any other RDP-compatible software. Please note, that although being installed already, you may need to log into Visual Studio with a Microsoft account (e.g., your @edu.bme.hu account). VM performance Please note that the performance of the VM depends on the usage of the cloud. You may experience a slowdown compared to a performant machine. The exercises can be solved on the VM in due time (we have tested it). You can help the VM but not running multiple Visual Studio instances at the same time and not opening the browser inside the VM.","title":"Using BME Cloud"},{"location":"mssql/exercise1/","text":"Exercise 1: Get the starter solution \u00b6 Submission guidelines Do not forget to follow the submission guidelines . Checking out your repository \u00b6 Open the following URL and accept the assignment to get your repository: https://classroom.github.com/a/HywjUNvO Wait for the repository creation to complete, then check out the repository. If in university computer laboratories you are not asked for credentials to log in to GitHub when checking out the repository, the operation may fail. This is likely due to the machine using someone else's GitHub credentials. Delete these credentials first (see here ), then retry the checkout. Create a new branch for the solution and work on this branch. Open the checked out folder, and type your Neptun code into the neptun.txt file. There should be a single line with the 6 characters of your Neptun code and nothing else in this file. Prepare and check the data \u00b6 Download the data files from here and extract this zip's contents into the data folder. We cannot include this in the starter repository due to the limitations of GitHub Template Repositories. Hence the manual step here. Open the CSV files to check their contents. You will work with these datasets. Please familiarize yourself with their structure and content.","title":"Exercise 1: Get the starter solution"},{"location":"mssql/exercise1/#exercise-1-get-the-starter-solution","text":"Submission guidelines Do not forget to follow the submission guidelines .","title":"Exercise 1: Get the starter solution"},{"location":"mssql/exercise1/#checking-out-your-repository","text":"Open the following URL and accept the assignment to get your repository: https://classroom.github.com/a/HywjUNvO Wait for the repository creation to complete, then check out the repository. If in university computer laboratories you are not asked for credentials to log in to GitHub when checking out the repository, the operation may fail. This is likely due to the machine using someone else's GitHub credentials. Delete these credentials first (see here ), then retry the checkout. Create a new branch for the solution and work on this branch. Open the checked out folder, and type your Neptun code into the neptun.txt file. There should be a single line with the 6 characters of your Neptun code and nothing else in this file.","title":"Checking out your repository"},{"location":"mssql/exercise1/#prepare-and-check-the-data","text":"Download the data files from here and extract this zip's contents into the data folder. We cannot include this in the starter repository due to the limitations of GitHub Template Repositories. Hence the manual step here. Open the CSV files to check their contents. You will work with these datasets. Please familiarize yourself with their structure and content.","title":"Prepare and check the data"},{"location":"mssql/exercise2/","text":"Exercise 2: Create database and table for users data \u00b6 Connect to the local database server using SQL Server Management Studio with the following credentials: Server name: (localdb)\\mssqllocaldb If you are using the Express edition, the connection string is localhost\\sqlexpress Authentication: Windows Authentication Use Object explorer to create a new database. The name should be your NEPTUN code. Create a new table to store data from the users csv file. The columns of the table should copy the columns of the csv file with one exception: the \"Location\" column should be split into \"City\" and \"Country.\" Click the New query button and execute the following script to create the table. CREATE TABLE \"BXUser\" ( \"UserID\" int , \"Age\" int , \"City\" nvarchar ( 1000 ), \"Country\" nvarchar ( 1000 ) )","title":"Exercise 2: Create database and table for users data"},{"location":"mssql/exercise2/#exercise-2-create-database-and-table-for-users-data","text":"Connect to the local database server using SQL Server Management Studio with the following credentials: Server name: (localdb)\\mssqllocaldb If you are using the Express edition, the connection string is localhost\\sqlexpress Authentication: Windows Authentication Use Object explorer to create a new database. The name should be your NEPTUN code. Create a new table to store data from the users csv file. The columns of the table should copy the columns of the csv file with one exception: the \"Location\" column should be split into \"City\" and \"Country.\" Click the New query button and execute the following script to create the table. CREATE TABLE \"BXUser\" ( \"UserID\" int , \"Age\" int , \"City\" nvarchar ( 1000 ), \"Country\" nvarchar ( 1000 ) )","title":"Exercise 2: Create database and table for users data"},{"location":"mssql/exercise3/","text":"Exercise 3: Import the users dataset using Integration Services \u00b6 Extend the provided Integration Services project ex3.sln by modifying the project contents in place. You can find this solution in the root of the checked out git repository. Open the solution with Visual Studio, then open Package.dtsx from Solution explorer . Find the SSIS Toolbox ; you will need to use this window in the following tasks. If you cannot see this toolbox window, after opening Package.dtsx , look for the Extensions > SSIS > SSIS Toolbox menu. Add a new SQL Task to the Control flow that erases any content from the target database table. You will likely need to run the ETL process multiple times, but the data should only be imported once, hence the initial cleanup task. Drag the SQL Task from the toolbox. Double click to open the details. Specify the connection settings: Use ADO.NET connection type. Add a new connection using the dropdown. This will open a new dialog, where you need to click on the New button again to add a new connection. Use the same connection settings (address and authentication) as before. When you configured the connection properties, make sure to press the Test button to verify the settings. Specify a truncate table <tablename> sql statement to prune the table contents. The sql command should NOT contain the database name; that is already configured in the connection manager. Add a new Data flow element to the control flow pane. Give a meaningful name to this task: \"import users.\" This task should follow the previous cleanup task: draw a line from the previous task box to this one to specify the ordering. Your control flow should like the following at this moment. Open the data flow element by double-clicking it. Drag a Flat file source from the toolbox to the data flow pane. Double click to open in. Add a new Flat file connection manager that specifies the source file and its details. Browse the input file BX-Users.csv from your local disk. Set the correct Format for parsing Format: Delimited Text qualifier: specify \" (double quotation mark) Check \"Column names in the first data row\" Switch to the Columns page (within the dialog) Verify that the Row delimiter is {CR}{LF} , and the Column delimiter is a semicolon. Switch to the Advanced page (within the dialog) Verify that the correct number of columns is present Change the UserID to be parsed as four byte signed integer Although the age should be a number, it must be parsed as a string here, because there are values in the CSV with the literal NULL text Increase the length ( OutputColumnWidth ) of the Location column from 50 to 1.000 The preview pane is a good way to verify the results. Ensure that there are no lingering quotation marks in the values (if there are, you missed to specify the text qualifier before). Close the dialog. If you need to modify the settings, you will find this Connection manager at the bottom of the designer surface. To split the Location column into two and parse the Age as a number, add a Derived column transformation (drag one from the SSIS Toolbox pane to the data flow editor surface). Connect the blue output of the file source into this new box to specify the data flow direction (see image below for reference). Then open the settings of the Derived column element to specify how the derived values are calculated. Add a new column that contains the age as an integer. Check if the string is \"NULL\", in that case, keep the null value, or cast to int. Use the following expression: [Age] == \"NULL\" ? NULL(DT_I4) : (DT_I4)[Age] . (You may need to change the column names.) Split the location column into two, and include your Neptun code in the text: create a new City and a new Country column using string operations; Country should be prefixed with your Neptun code. Take a look at a few examples of the Location : they are in the form of \"city, state, country\" To separate the country, find the second comma in the text. The rest is the city (you can keep the state included in the city text). Prefix the country with your Neptun code by concatenating it as a string to the front. E.g., \"nyc, new york, usa\" becomes: Country: \"NEPTUN usa\" City: \"nyc, new york\" Find more information on the syntax of the Expression here: https://docs.microsoft.com/en-us/sql/integration-services/expressions/integration-services-ssis-expressions?view=sql-server-2017) Remove duplicate records. Add a new Sort transformation, then connect it to the output of the previous element. Open its settings: chose to sort on the UserID , and check the Remove rows with duplicate sort values checkbox. Add an ADO NET Destination component (look in the Other Destinations category) to save the data into the database. Direct the output of the previous Sort component here. Open the component settings dialog by double-clicking the component box. Use the connection manager created before. (Make sure not to create a new connection manager - the database access has already been defined in the existing connection manager.) Select the users table you created as the target. Check the mapping and make sure that the right fields go into the right columns. Make sure to double-check the Age column because it may exist as a number, and as a string - you need the numbered type here. (Mind, that the input column names may be different for you.) If you were to execute the process now, it would fail due to errors in the Derived columns transformation. Some content does not correspond to the transformations. Let us skip these rows. Add another ADO NET Destination component. Point the red arrow from the Derived column into this new database destination. A the Configure Error Output dialog will pop up to specify how to handle errors. For all columns and error types select Redirect row . There is a total number of 6 elements to change here. Close this dialog. Open the new ADO NET Destination and specify its settings too. Although the result of the transformations is not available, we still save the UserId value. The other database columns will not have any mapping. So effectively, only the UserID will be saved to the database. Use the connection manager created before. (Make sure not to create a new connection manager - the database access has already been defined in the existing connection manager.) Run the ETL process by using the Start button on the toolbar of Visual Studio. Verify that the execution succeeded. If not, see this guide for debugging and resolving typical issues. If the process succeeded, verify the contents of the table using SQL Server Management Studio. Verify that you added your Neptun code prefix to the Country names. There will be a few rows with NULL values; these records failed the Derived column transformation. Check whether the failure was justified (the problem is with the input), or whether the error is in your ETL process! SUBMISSION Create a screenshot of the database table contents. Save the screenshot file as ex3.png - overwrite the placeholder file with yours. Please make sure that the screenshot is taken such that it includes the database name (which is your Neptun code) from the Object explorer window, and includes a few sample records with the Neptun code clearly visible.","title":"Exercise 3: Import the users dataset using Integration Services"},{"location":"mssql/exercise3/#exercise-3-import-the-users-dataset-using-integration-services","text":"Extend the provided Integration Services project ex3.sln by modifying the project contents in place. You can find this solution in the root of the checked out git repository. Open the solution with Visual Studio, then open Package.dtsx from Solution explorer . Find the SSIS Toolbox ; you will need to use this window in the following tasks. If you cannot see this toolbox window, after opening Package.dtsx , look for the Extensions > SSIS > SSIS Toolbox menu. Add a new SQL Task to the Control flow that erases any content from the target database table. You will likely need to run the ETL process multiple times, but the data should only be imported once, hence the initial cleanup task. Drag the SQL Task from the toolbox. Double click to open the details. Specify the connection settings: Use ADO.NET connection type. Add a new connection using the dropdown. This will open a new dialog, where you need to click on the New button again to add a new connection. Use the same connection settings (address and authentication) as before. When you configured the connection properties, make sure to press the Test button to verify the settings. Specify a truncate table <tablename> sql statement to prune the table contents. The sql command should NOT contain the database name; that is already configured in the connection manager. Add a new Data flow element to the control flow pane. Give a meaningful name to this task: \"import users.\" This task should follow the previous cleanup task: draw a line from the previous task box to this one to specify the ordering. Your control flow should like the following at this moment. Open the data flow element by double-clicking it. Drag a Flat file source from the toolbox to the data flow pane. Double click to open in. Add a new Flat file connection manager that specifies the source file and its details. Browse the input file BX-Users.csv from your local disk. Set the correct Format for parsing Format: Delimited Text qualifier: specify \" (double quotation mark) Check \"Column names in the first data row\" Switch to the Columns page (within the dialog) Verify that the Row delimiter is {CR}{LF} , and the Column delimiter is a semicolon. Switch to the Advanced page (within the dialog) Verify that the correct number of columns is present Change the UserID to be parsed as four byte signed integer Although the age should be a number, it must be parsed as a string here, because there are values in the CSV with the literal NULL text Increase the length ( OutputColumnWidth ) of the Location column from 50 to 1.000 The preview pane is a good way to verify the results. Ensure that there are no lingering quotation marks in the values (if there are, you missed to specify the text qualifier before). Close the dialog. If you need to modify the settings, you will find this Connection manager at the bottom of the designer surface. To split the Location column into two and parse the Age as a number, add a Derived column transformation (drag one from the SSIS Toolbox pane to the data flow editor surface). Connect the blue output of the file source into this new box to specify the data flow direction (see image below for reference). Then open the settings of the Derived column element to specify how the derived values are calculated. Add a new column that contains the age as an integer. Check if the string is \"NULL\", in that case, keep the null value, or cast to int. Use the following expression: [Age] == \"NULL\" ? NULL(DT_I4) : (DT_I4)[Age] . (You may need to change the column names.) Split the location column into two, and include your Neptun code in the text: create a new City and a new Country column using string operations; Country should be prefixed with your Neptun code. Take a look at a few examples of the Location : they are in the form of \"city, state, country\" To separate the country, find the second comma in the text. The rest is the city (you can keep the state included in the city text). Prefix the country with your Neptun code by concatenating it as a string to the front. E.g., \"nyc, new york, usa\" becomes: Country: \"NEPTUN usa\" City: \"nyc, new york\" Find more information on the syntax of the Expression here: https://docs.microsoft.com/en-us/sql/integration-services/expressions/integration-services-ssis-expressions?view=sql-server-2017) Remove duplicate records. Add a new Sort transformation, then connect it to the output of the previous element. Open its settings: chose to sort on the UserID , and check the Remove rows with duplicate sort values checkbox. Add an ADO NET Destination component (look in the Other Destinations category) to save the data into the database. Direct the output of the previous Sort component here. Open the component settings dialog by double-clicking the component box. Use the connection manager created before. (Make sure not to create a new connection manager - the database access has already been defined in the existing connection manager.) Select the users table you created as the target. Check the mapping and make sure that the right fields go into the right columns. Make sure to double-check the Age column because it may exist as a number, and as a string - you need the numbered type here. (Mind, that the input column names may be different for you.) If you were to execute the process now, it would fail due to errors in the Derived columns transformation. Some content does not correspond to the transformations. Let us skip these rows. Add another ADO NET Destination component. Point the red arrow from the Derived column into this new database destination. A the Configure Error Output dialog will pop up to specify how to handle errors. For all columns and error types select Redirect row . There is a total number of 6 elements to change here. Close this dialog. Open the new ADO NET Destination and specify its settings too. Although the result of the transformations is not available, we still save the UserId value. The other database columns will not have any mapping. So effectively, only the UserID will be saved to the database. Use the connection manager created before. (Make sure not to create a new connection manager - the database access has already been defined in the existing connection manager.) Run the ETL process by using the Start button on the toolbar of Visual Studio. Verify that the execution succeeded. If not, see this guide for debugging and resolving typical issues. If the process succeeded, verify the contents of the table using SQL Server Management Studio. Verify that you added your Neptun code prefix to the Country names. There will be a few rows with NULL values; these records failed the Derived column transformation. Check whether the failure was justified (the problem is with the input), or whether the error is in your ETL process! SUBMISSION Create a screenshot of the database table contents. Save the screenshot file as ex3.png - overwrite the placeholder file with yours. Please make sure that the screenshot is taken such that it includes the database name (which is your Neptun code) from the Object explorer window, and includes a few sample records with the Neptun code clearly visible.","title":"Exercise 3: Import the users dataset using Integration Services"},{"location":"mssql/exercise4/","text":"Exercise 4: Report about users \u00b6 Extend the provided Reporting Services project ex4.sln by modifying the project contents in place. You can find this solution in the root of the checked out git repository. Create a report that summarizes from which country do our users come from. A table should list the number of users by country, along with the average age from the country, and a pie chart should display the number of users from the top 10 countries. The final report should look similar to this: Open the solution with Visual Studio, and open users.rdl from Solution explorer . Locate the Report data toolbox. If you cannot find it, open it from the View menu. Add a new Data source to the report by right-clicking the Data sources folder in the Report data window. Chose the \"Embedded connection\" option. Connect to Microsoft SQL Server . Use the Edit button to specify connection details. Use the same settings as before. Switch to the Credentials page and select the Use Windows Authentication option . After the data source is added, open its properties and verify the Credentials . You might need to set up this page again. (This seems to be a bug in Visual Studio.) Add a new dataset by right-clicking the Datasets folder in the Report data window. The dataset specifies how to fetch the data we want to display. It will be an SQL query. Chose \"Use a dataset embedded in my report\" Select the data source from the dropdown. It is the data source created in the previous step. The query should list the number of users and the average age for each country sorted descending according to the number of users. The query should also skip countries that are \"outliers\" having too few (<50) users. Write the query as text as follows select Country , count ( * ) as Count , avg ( Age ) as AvgAge from BXUser where Country is not NULL and Country <> '' group by Country having count ( * ) >= 50 order by Count desc Visualize the result in a table. Add a table to the report surface by dragging one from the Toolbox . Drag each field from the Report data toolbox from under the Dataset created above into the table. Delete obsolete columns by right-clicking the table. There should be 3 columns: country, number of users, and average age. Edit the header row to have meaningful column headers. Add a pie chart by dragging a Chart item from the Toolbox next to the table. The designer view of the report should be similar: Chose a pie chart type. Create a new dataset, similar to the one before, that returns only the top 10 rows. Open the Chart data overlay by clicking the chart component. You need to click to a white area for this overlay to appear. From the newly created dataset display the Count field as the Values and the Country field as Category groups : Add a title to the report, and include your Neptun code in the title . (See the sample report at the top.) Preview the report by switching to the Preview tab in Visual Studio. SUBMISSION Include a screenshot of the report as ex4.png . Please make sure that the screenshot is taken such that relevant parts of the report are all visible (title, table, and diagram too). See the sample at the beginning of the exercise.","title":"Exercise 4: Report about users"},{"location":"mssql/exercise4/#exercise-4-report-about-users","text":"Extend the provided Reporting Services project ex4.sln by modifying the project contents in place. You can find this solution in the root of the checked out git repository. Create a report that summarizes from which country do our users come from. A table should list the number of users by country, along with the average age from the country, and a pie chart should display the number of users from the top 10 countries. The final report should look similar to this: Open the solution with Visual Studio, and open users.rdl from Solution explorer . Locate the Report data toolbox. If you cannot find it, open it from the View menu. Add a new Data source to the report by right-clicking the Data sources folder in the Report data window. Chose the \"Embedded connection\" option. Connect to Microsoft SQL Server . Use the Edit button to specify connection details. Use the same settings as before. Switch to the Credentials page and select the Use Windows Authentication option . After the data source is added, open its properties and verify the Credentials . You might need to set up this page again. (This seems to be a bug in Visual Studio.) Add a new dataset by right-clicking the Datasets folder in the Report data window. The dataset specifies how to fetch the data we want to display. It will be an SQL query. Chose \"Use a dataset embedded in my report\" Select the data source from the dropdown. It is the data source created in the previous step. The query should list the number of users and the average age for each country sorted descending according to the number of users. The query should also skip countries that are \"outliers\" having too few (<50) users. Write the query as text as follows select Country , count ( * ) as Count , avg ( Age ) as AvgAge from BXUser where Country is not NULL and Country <> '' group by Country having count ( * ) >= 50 order by Count desc Visualize the result in a table. Add a table to the report surface by dragging one from the Toolbox . Drag each field from the Report data toolbox from under the Dataset created above into the table. Delete obsolete columns by right-clicking the table. There should be 3 columns: country, number of users, and average age. Edit the header row to have meaningful column headers. Add a pie chart by dragging a Chart item from the Toolbox next to the table. The designer view of the report should be similar: Chose a pie chart type. Create a new dataset, similar to the one before, that returns only the top 10 rows. Open the Chart data overlay by clicking the chart component. You need to click to a white area for this overlay to appear. From the newly created dataset display the Count field as the Values and the Country field as Category groups : Add a title to the report, and include your Neptun code in the title . (See the sample report at the top.) Preview the report by switching to the Preview tab in Visual Studio. SUBMISSION Include a screenshot of the report as ex4.png . Please make sure that the screenshot is taken such that relevant parts of the report are all visible (title, table, and diagram too). See the sample at the beginning of the exercise.","title":"Exercise 4: Report about users"},{"location":"mssql/exercise5/","text":"Exercise 5: Import the books and ratings datasets using Integration Services \u00b6 Your task is to import the remaining two CSV files using Integration Services into the database. Create two new tables in the database to store the books and ratings contents. Design the tables such that they map to the columns in the CSV. SUBMISSION Put the SQL code that creates the tables into two separate files ex5_books.sql and ex5_ratings.sql . There should be a single sql command (\"create table\") in each file. Please do not include the database name in the commands and do not include any go command in them either. Create a new ETL process to import the books and ratings datasets. Open the provided empty Integration Services project in ex5.sln , and perform the modifications in place. Import the ISBN from the books as string . Prefix the book titles with your Neptun code. You will need to filter for duplicate ISBNs among the books dataset, as we did before. Make sure to use case insensitive comparison for the duplicate filter. Make sure to import the image URLs from the books data file. Remember to extend the imported columns' length in the flat file connection settings ( OutputColumnWidth ) for the fields; 1000 characters should be sufficient. The ratings dataset contains invalid values: there are rows where the value is 0 . These should be skipped using a Conditional split component. It is best to separate the two CSV import into different data flows within the same control flow process. SUBMISSION Include proof of the successful import by taking a screenshot of the books database table contents after successfully executing the ETL process as ex5.png . Please make sure that the screenshot is taken such that it includes the database name (which is your Neptun code) from the Object explorer window, and includes a few sample records with the Neptun code clearly visible.","title":"Exercise 5: Import the books and ratings datasets using Integration Services"},{"location":"mssql/exercise5/#exercise-5-import-the-books-and-ratings-datasets-using-integration-services","text":"Your task is to import the remaining two CSV files using Integration Services into the database. Create two new tables in the database to store the books and ratings contents. Design the tables such that they map to the columns in the CSV. SUBMISSION Put the SQL code that creates the tables into two separate files ex5_books.sql and ex5_ratings.sql . There should be a single sql command (\"create table\") in each file. Please do not include the database name in the commands and do not include any go command in them either. Create a new ETL process to import the books and ratings datasets. Open the provided empty Integration Services project in ex5.sln , and perform the modifications in place. Import the ISBN from the books as string . Prefix the book titles with your Neptun code. You will need to filter for duplicate ISBNs among the books dataset, as we did before. Make sure to use case insensitive comparison for the duplicate filter. Make sure to import the image URLs from the books data file. Remember to extend the imported columns' length in the flat file connection settings ( OutputColumnWidth ) for the fields; 1000 characters should be sufficient. The ratings dataset contains invalid values: there are rows where the value is 0 . These should be skipped using a Conditional split component. It is best to separate the two CSV import into different data flows within the same control flow process. SUBMISSION Include proof of the successful import by taking a screenshot of the books database table contents after successfully executing the ETL process as ex5.png . Please make sure that the screenshot is taken such that it includes the database name (which is your Neptun code) from the Object explorer window, and includes a few sample records with the Neptun code clearly visible.","title":"Exercise 5: Import the books and ratings datasets using Integration Services"},{"location":"mssql/exercise6/","text":"Exercise 6: Create a report listing the highest rated books \u00b6 In this exercise, you are required to create a new report using Reporting Services. Create a report that lists the top 10 highest rated books in a table. The report should be edited in solution ex6.sln in file books.rdl . The top 10 books should be presented as a table displaying the title, author, publication year, average rating, and the cover image of the book (using the medium-sized image URL from the dataset). Use an Image component from the toolbox. The image source should be External to fetch it based on the URL. An SQL script should compute the average rating. Ignore any book that does not have at least 20 ratings. Include this sql script (a single select sql statement) in file ex6.sql . The file should contain a single sql query. Do not include the database name, and do not include any go command either. Make sure to calculate the rating appropriately: if the imported rating is of type integer, make sure to use decimal types when averaging the values! Add a title to the report, and include your Neptun code in the title. The final report should look similar to this (your results do not need to match exactly): SUBMISSION Include a screenshot of the report in file ex6.png . Please ensure that the screenshot is taken so that relevant parts of the report are all visible (title, table with Neptun codes, ratings, cover images). See the sample above.","title":"Exercise 6: Create a report listing the highest rated books"},{"location":"mssql/exercise6/#exercise-6-create-a-report-listing-the-highest-rated-books","text":"In this exercise, you are required to create a new report using Reporting Services. Create a report that lists the top 10 highest rated books in a table. The report should be edited in solution ex6.sln in file books.rdl . The top 10 books should be presented as a table displaying the title, author, publication year, average rating, and the cover image of the book (using the medium-sized image URL from the dataset). Use an Image component from the toolbox. The image source should be External to fetch it based on the URL. An SQL script should compute the average rating. Ignore any book that does not have at least 20 ratings. Include this sql script (a single select sql statement) in file ex6.sql . The file should contain a single sql query. Do not include the database name, and do not include any go command either. Make sure to calculate the rating appropriately: if the imported rating is of type integer, make sure to use decimal types when averaging the values! Add a title to the report, and include your Neptun code in the title. The final report should look similar to this (your results do not need to match exactly): SUBMISSION Include a screenshot of the report in file ex6.png . Please ensure that the screenshot is taken so that relevant parts of the report are all visible (title, table with Neptun codes, ratings, cover images). See the sample above.","title":"Exercise 6: Create a report listing the highest rated books"}]}